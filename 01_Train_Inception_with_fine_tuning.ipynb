{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on separate images (CNN-Only) on InceptionV3 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_utils import set_keras_session\n",
    "set_keras_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'UCF11'\n",
    "nb_classes = int(dataset[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(images):\n",
    "\n",
    "    # InceptionV3 requires images to be in range -1 to 1.\n",
    "    return ((images / 255.) - 0.5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(dataset_dir, batch_size):\n",
    "    \n",
    "    all_files = os.listdir(dataset_dir)\n",
    "    \n",
    "    while True:\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            \n",
    "            random_filename = random.choice(all_files)\n",
    "            \n",
    "            with h5py.File(join(dataset_dir, random_filename), 'r') as hf:\n",
    "                frames = hf['X'][:]\n",
    "                fr_labels = hf['Y'][:]\n",
    "                random_idx = np.random.randint(frames.shape[0])\n",
    "                \n",
    "                images.append(frames[random_idx])\n",
    "                labels.append(fr_labels)\n",
    "                \n",
    "        assert len(labels) == len(images)\n",
    "        \n",
    "        images = np.array(images)\n",
    "        labels = to_categorical(np.array(labels), nb_classes)\n",
    "        \n",
    "        yield preprocess_images(images), labels\n",
    "        \n",
    "        \n",
    "\n",
    "def valid_generator(dataset_dir, batch_size):\n",
    "    \n",
    "    all_files = os.listdir(dataset_dir)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        for filename in all_files:\n",
    "\n",
    "            with h5py.File(join(dataset_dir, filename), 'r') as hf:\n",
    "                frames = hf['X'][:]\n",
    "                single_label = hf['Y'][:][0]\n",
    "\n",
    "                fr_labels = np.array([single_label] * frames.shape[0])\n",
    "\n",
    "                yield preprocess_images(frames), to_categorical(fr_labels, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is (240, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "separate_dataset_dir = join('datasets', dataset, 'separate_frames_50_h_240_w_320')\n",
    "train_dir = join(separate_dataset_dir, 'train')\n",
    "valid_dir = join(separate_dataset_dir, 'valid')\n",
    "\n",
    "train_samples_count = len(os.listdir(train_dir))\n",
    "valid_samples_count = len(os.listdir(valid_dir))\n",
    "\n",
    "with h5py.File(join(train_dir, os.listdir(train_dir)[0])) as hf:\n",
    "    image_shape = hf['X'][:].shape[1:]\n",
    "    #inception_shape = hf['inception'][:].shape[1:]\n",
    "    print('Image shape is', image_shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_layer = layers.Input(inception_shape)\n",
    "x = input_layer\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = layers.Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "from keras import metrics\n",
    "from functools import partial\n",
    "\n",
    "top_3_k_categorical_accuracy = partial(metrics.top_k_categorical_accuracy, k=3)\n",
    "top_3_k_categorical_accuracy.__name__ = 'top_3'\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', \n",
    "              metrics=['accuracy', top_3_k_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "369/369 [==============================] - 324s 878ms/step - loss: 0.6069 - acc: 0.8132 - top_3: 0.9386 - val_loss: 1.2736 - val_acc: 0.6510 - val_top_3: 0.9064\n",
      "Epoch 2/3\n",
      "369/369 [==============================] - 320s 868ms/step - loss: 0.2236 - acc: 0.9344 - top_3: 0.9900 - val_loss: 1.6638 - val_acc: 0.6345 - val_top_3: 0.9032\n",
      "Epoch 3/3\n",
      "369/369 [==============================] - 408s 1s/step - loss: 0.1593 - acc: 0.9508 - top_3: 0.9944 - val_loss: 1.5083 - val_acc: 0.6701 - val_top_3: 0.9016\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator(train_dir, batch_size),\n",
    "                    steps_per_epoch=train_samples_count * 50 // batch_size, \n",
    "                    validation_data=valid_generator(valid_dir, batch_size),\n",
    "                    validation_steps=valid_samples_count,\n",
    "                    epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(generator, how_many):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take the first how_many results from a generator \n",
    "    (or less, if the generator won't generate as many results).\n",
    "    Note that this function is also a generator.\n",
    "    \"\"\"\n",
    "    \n",
    "    for _, res in zip(range(how_many), generator):\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we collect metrics about the accuracy on the video, since the model only works with single images:\n",
    "* Approach 1: Compute the mean of all predictions on all frames of a video, then take the best prediction.\n",
    "* Approach 2: Take the predicted best for each frame (argmax), and count how many times that class is predicted as the best one into the video frames. Then take the class with the highest count.\n",
    "* Approach 3 (not implemented but tested): instead of the mean, compute the product (since we work with probabilities). Yields a lower result than using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count_videos = 0\n",
    "count_top_1 = 0\n",
    "count_top_3 = 0\n",
    "count_top_1_argmax = 0\n",
    "\n",
    "for frames, labels in take(valid_generator(valid_dir, batch_size), valid_samples_count):\n",
    "    \n",
    "    ### Compute the predicted labels using the model\n",
    "    \n",
    "    true_labels = labels[0]\n",
    "    true_label_idx = np.argmax(true_labels)\n",
    "    predicted_labels = model.predict(frames)\n",
    "    \n",
    "    ### Update counters with Approach 1 (mean)\n",
    "    \n",
    "    predicted_labels_mean = np.mean(predicted_labels, axis=0)\n",
    "    predicted_labels_mean_idx = np.argmax(predicted_labels_mean)\n",
    "    idx_sorted_top_3 = np.argsort(predicted_labels_mean)[-3:]\n",
    "    \n",
    "    if true_label_idx in idx_sorted_top_3:\n",
    "        count_top_3 += 1\n",
    "        \n",
    "    if true_label_idx == predicted_labels_mean_idx:\n",
    "        count_top_1 += 1\n",
    "        \n",
    "    ### Update count with Approach 2 (highest count)\n",
    "    \n",
    "    predicted_labels_argmax = np.argmax(predicted_labels, axis=1)\n",
    "    counter = Counter(predicted_labels_argmax)\n",
    "    \n",
    "    if counter.most_common(1)[0][0] == true_label_idx:\n",
    "        count_top_1_argmax += 1\n",
    "        \n",
    "    ### Update number of videos\n",
    "        \n",
    "    count_videos += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy (using mean): 0.7003154574132492\n",
      "Top 3 accuracy (using mean): 0.9526813880126183\n",
      "Top 1 accuracy (using highest count): 0.6971608832807571\n"
     ]
    }
   ],
   "source": [
    "print('Top 1 accuracy (using mean):', count_top_1 / count_videos)\n",
    "print('Top 3 accuracy (using mean):', count_top_3 / count_videos)\n",
    "print('Top 1 accuracy (using highest count):', count_top_1_argmax / count_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018/03/01: Training with images with full resolution (240 x 320) results in better accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
